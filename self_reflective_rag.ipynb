{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNITfxgCPxNZEC7yeFN3g9r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranabilal09/Self-Corrective-Rag-Chatbot/blob/main/self_reflective_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "L24NYaTdRCJa"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade langchain langgraph langchain_community langchain-google-genai langchain_chroma langchain_huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['LANGCHAIN_PROJECT'] = \"self-reflective-rag\"\n",
        "os.environ[\"LANGCHAI_TRACING_V2\"] = \"true\"\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('langchai_api_key')\n",
        "google_api_key = userdata.get('Gemini_Api_Key')\n",
        "os.environ['GOOGLE_API_KEY'] = google_api_key\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('tavily_api_key')\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "6E971dneRxmu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "#load\n",
        "urls = [\"https://python.langchain.com/docs/tutorials/graph/\",\n",
        "        \"https://python.langchain.com/docs/how_to/#retrievers\",\n",
        "        \"https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/\"]\n",
        "\n",
        "docs = [WebBaseLoader(url).load_and_split() for url in urls]\n",
        "docs_split = [items for sublist in docs for items in sublist]\n",
        "\n",
        "#split\n",
        "spliting = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs_split = spliting.split_documents(docs_split)\n",
        "\n",
        "#embedding\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "#vectorestore\n",
        "\n",
        "vectorstore = Chroma.from_documents(docs_split, embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "Nx6IrqDrTqFZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "QtdBOgUVY7MB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict , Dict\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  keys: Dict[str , any]"
      ],
      "metadata": {
        "id": "EAr4-nlcZvOA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser , PydanticOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel , Field\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "from langchain_core.tools import tool\n",
        "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "#nodes\n",
        "\n",
        "def retrieve(state):\n",
        "  print(\"----RETRIEVE----\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = retriever.get_relevant_documents(question)\n",
        "  return {\"keys\": {\"documents\": documents , \"question\": question}}\n",
        "\n",
        "def generation(state):\n",
        "  print(\"----GENERATION----\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = retriever.get_relevant_documents(question)\n",
        "\n",
        "  #prompt\n",
        "  prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "  #llm\n",
        "  llm = ChatGoogleGenerativeAI(\n",
        "      model= \"gemini-1.5-flash-8b\"\n",
        "  )\n",
        "  rag_chain = prompt | llm | StrOutputParser()\n",
        "  #generation\n",
        "  generation = rag_chain.invoke({\"context\": documents , \"question\": question})\n",
        "  return {\"keys\": {\"generation\": generation , \"documents\" : documents , \"question\": question}}\n",
        "\n",
        "def grade_documents(state):\n",
        "  print(\"----Check Relevance----\")\n",
        "  state_dict= state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = state_dict[\"documents\"]\n",
        "\n",
        "  class grade(BaseModel):\n",
        "    \"\"\" check the relevance documents\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=(\"Check binary score 'yes' or 'no' \")\n",
        "    )\n",
        "\n",
        "  #llm\n",
        "  llm = ChatGoogleGenerativeAI(\n",
        "    model= \"gemini-1.5-flash-8b\"\n",
        "  )\n",
        "\n",
        "  #prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template= \"\"\"You are a grader accessing relevance of retrieved documents to the user question.\\n\n",
        "      Here is the retrieved documnets:\\n{context}.\\n\n",
        "      Here is the user Question:{question}.\\n\n",
        "      If the documents contain keyword(s) or semantic meaning relative to the user question ,grade them as relevant.\\n\n",
        "      Give a relevance score 'yes' or 'no' score for all documents to indicate that weather all documents are relevant.\\n\n",
        "      your response should be in json format:\"\"\",\n",
        "      input_variables=[\"context\" , \"question\"]\n",
        "  )\n",
        "\n",
        "  chain= prompt | llm.with_structured_output(grade,include_raw=True)\n",
        "\n",
        "  search = \"no\"\n",
        "  filtered_docs=[]\n",
        "  for d in documents:\n",
        "    score = chain.invoke({\"context\": d.page_content , \"question\": question})\n",
        "    if isinstance(score, tuple) and len(score) > 0 and hasattr(score[0], 'binary_score'):\n",
        "      if score[0].binary_score == \"yes\":\n",
        "            filtered_docs.append(d)\n",
        "      else:\n",
        "            print(\"----Documents are not relevant\")\n",
        "            search = \"yes\"\n",
        "    else:\n",
        "        # If structured output parsing failed, print message and potentially log the issue\n",
        "        print(\"----Could not parse relevance score, skipping document\")\n",
        "        # Optionally, you can log the raw 'score' value for debugging\n",
        "  return {\"keys\": {\"documents\": filtered_docs , \"question\": question , \"search\": search}}"
      ],
      "metadata": {
        "id": "chHnpOZybjeX"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain.schema import Document\n",
        "\n",
        "def translate_query(state):\n",
        "  print(\"----Translate Query----\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = state_dict[\"documents\"]\n",
        "\n",
        "  #llm\n",
        "  llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-8b\")\n",
        "\n",
        "  #prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"\"\" You are generating question that is well optimized for retrieval.\\n\n",
        "      Look at the input and try to reason about underlying semantic intent / meanings.\\n\n",
        "      Here is the initial question:\n",
        "      \\n-------\\n\n",
        "      {question}\n",
        "      \\n-------\\n\n",
        "      Formulate an improved question:\"\"\",\n",
        "      input_variables=[\"question\"]\n",
        "  )\n",
        "\n",
        "  #chain\n",
        "  chain= prompt | llm | StrOutputParser()\n",
        "  new_question = chain.invoke({\"question\": question})\n",
        "  return {\"keys\": {\"question\": new_question , \"documents\": documents}}\n",
        "\n",
        "def web_search(state):\n",
        "  print(\"----Web Search----\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = state_dict[\"documents\"]\n",
        "\n",
        "  tavily = TavilySearchResults(max_results=1)\n",
        "  tavily_search= tavily.invoke(question)\n",
        "  web_results = \"\\n\".join([d[\"content\"] for d in tavily_search])\n",
        "  web_results = Document(page_content=web_results)\n",
        "  documents.append(web_results)\n",
        "  return {\"keys\": {\"documents\": documents , \"question\": question}}\n",
        "\n",
        "def decide(state):\n",
        "  print(\"----Decide----\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = state_dict[\"documents\"]\n",
        "  search = state_dict[\"search\"]\n",
        "\n",
        "  if search == \"yes\":\n",
        "    print(\"----DECISION: Translate Query and Search the Web\")\n",
        "    return \"translate\"\n",
        "  else:\n",
        "    print(\"----DECISION: Generation\")\n",
        "    return \"generation\"\n",
        "\n"
      ],
      "metadata": {
        "id": "DbMBw0P0sN6U"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph ,END\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "graph.add_node(\"retrieve\" , retrieve) #retrieve\n",
        "graph.add_node(\"generation\", generation) #generation\n",
        "graph.add_node(\"grade_documents\", grade_documents) #grade_documents\n",
        "graph.add_node(\"translate_query\", translate_query) #translate\n",
        "graph.add_node(\"web_search\", web_search) #web_search\n",
        "\n",
        "\n",
        "graph.set_entry_point(\"retrieve\")\n",
        "graph.add_edge(\"retrieve\" , \"grade_documents\")\n",
        "graph.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide,\n",
        "    {\n",
        "        \"translate\": \"translate_query\",\n",
        "        \"generation\": \"generation\"\n",
        "    }\n",
        ")\n",
        "graph.add_edge(\"translate_query\", \"web_search\" )\n",
        "graph.add_edge(\"web_search\",\"generation\")\n",
        "graph.add_edge(\"generation\", END)\n",
        "\n",
        "app = graph.compile()"
      ],
      "metadata": {
        "id": "kx7KeS4xwlWx"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = \"how to retrieve relevant documents from vectorestore using langgraph?\"\n",
        "for output in app.stream({\"keys\": {\"question\":inputs}},{\"recursion_limit\":150}):\n",
        "  for key, value in output.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "1s_c18SL00jL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}